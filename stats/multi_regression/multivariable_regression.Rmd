---
title: "Advanced statistics in R"
subtitle: "Regression - variable selection and multivariable regression"
author: ""
date: '[contact@appliedepi.org](mailto:contact@appliedepi.org)'
output:
  xaringan::moon_reader:
    seal: TRUE
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    css: [xaringan-themer.css] 
---

```{r, eval=F, echo=F, include = F}
# Must do in order to render.

pacman::p_load(xaringan,
               glmnet)

devtools::install_github("gadenbuie/xaringanExtra")
devtools::install_github("gadenbuie/countdown")
remotes::install_github("mitchelloharawild/icons")
icons::download_fontawesome()

# Render with xaringan::infinite_moon_reader()
# Slides will appear in viewer, and will update as you edit/save
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.retina = 3  ## retina more effective than w/h (according to twitter)
                      # fig.width = 16, 
                      # fig.height = 10
                      )
## I dont know why this is included in the example xaringan slide 
## but is something to do with background images
options(htmltools.dir.version = FALSE)

## install and load necessary packages 
pacman::p_load(
  rio,        # importing data  
  here,       # relative file pathways  
  janitor,    # data cleaning and tables
  lubridate,  # working with dates
  tidyverse,  # data management and visualization
  gtsummary,  # summary tables
  flair,      # coloring text
  kableExtra, # for output tables
  xaringanthemer,  # for styling presentation 
  caret,
  lme4,
  broom.mixed,
  countdown
)

library(countdown)
```


```{r  xaringan-themer, include = FALSE}

## define presentation colours (theme) using {xaringanthemer} package 
## https://pkg.garrickadenbuie.com/xaringanthemer/articles/xaringanthemer.html

## epirhandbook logo colours: 
  ## blue: "#00538c"
  ## green: "#007732"
  ## lighter green: "#48a878"

## see ?style_mono_accent for all the things can customise
style_mono_accent(
  base_color = "#00538c", 
  link_color = "#48a878", 
  # add logo to the title page (bit bigger)
  title_slide_background_image = "https://github.com/appliedepi/intro_course/raw/main/images/logo.png",
  title_slide_background_position = "95% 95%",
  title_slide_background_size = "25%",
  ## add logo to all following slides
  background_image = "https://github.com/appliedepi/intro_course/raw/main/images/logo.png", 
  background_size = "10%",
  background_position = "100% 0%"
)
```

```{css, echo=F}
    .remark-slide table{
      border: none
    }
    .remark-slide-table {
      
    }
    tr:first-child {
      border-top: none;
  }
    tr:last-child {
    border-bottom: none;
  }
    .tiny .remark-code { /*Change made here*/
  font-size: 50% !important;
    }
    .teenytiny .remark-code { /*Change made here*/
  font-size: 25% !important;
    }
    .giant .remark-code { /*Change made here*/
  font-size: 150% !important;
    }
    .smaller .remark-code { /*Change made here*/
  font-size: 75% !important;
    }
    .huge .remark-code { /*Change made here*/
  font-size: 250% !important;
    }
  
```


```{r, echo=F, eval=T}
set.seed(1)
surv <- rio::import(xfun::relative_path(here::here("data", "linelist_combined_20141201.rds")))

linelist <- surv %>%
     mutate(outcome_death = ifelse(outcome == "Death", 1, 0)) %>%
     filter(between(bmi, 10, 50)) %>%
  select(-c(district_res, district_det, district))

#This makes our tables compact so they dont get to large on slides
theme_gtsummary_compact()

```

# Multivariable regression

Now that you are experts at conducting univariate regression in R, it is time to move onto multivariable regression!

---

# Multivariable regression - when univariate is just not enough

Multivariable regression allows us to look at the relationship between our dependent variable and multiple independent variables. 

Why multivariable?

* It considers the relationship of each exposure _in context_ with the outcome
* This allows us to understand which exposures have a *true association* with the outcome and are not just subject to an *effect modification*

---

# Effect modification and confounding

Effect modification is when the **magnitude** of the effect of an exposure on an outcome differs by another exposure.

Confounding is when another exposure is related to both the exposure and the outcome, but **it does not lie on the causative pathway**.

Can we think of any examples of effect modification and confounding?

---

# Effect modification

Effect modification is all about stratification.

Which of these are examples of effect modification?

---

# Effect modification

Effect modification is all about stratification.

Which of these are examples of effect modification?

- When a drug works on females but does not work on males

---

# Effect modification

Effect modification is all about stratification.

Which of these are examples of effect modification?

- When a drug works on females but does not work on males
- Those who are exposed to asbestos **and** smoke have higher risks of lung cancer

---

# Effect modification

Effect modification is all about stratification.

Which of these are examples of effect modification?

- When a drug works on females but does not work on males
- Those who are exposed to asbestos **and** smoke have higher risks of lung cancer
- Alcohol consumption is associated with lung cancer, those who drink alcohol are more likely to smoke

---

# Confounding

A confounder is a variable that influences both the dependent variable and the independent variable, causing a spurious association.

```{r, eval = TRUE, echo = FALSE, out.width = "125%"}
knitr::include_graphics(xfun::relative_path(here::here("images", "regression", "confounder_wikipedia.png")))
```
https://upload.wikimedia.org/wikipedia/commons/5/53/Comparison_confounder_mediator.svg


---

# Confounding

So icecream is causing sharks to bite humans?

```{r, eval = TRUE, echo = FALSE, out.width = "100%"}
knitr::include_graphics(xfun::relative_path(here::here("images", "regression", "icecream_sharks.png")))
```

---
# Effect modification and confounding

Multivariable regression estimates the association between the exposures and the outcome by holding all other exposures constant.

This allows us to account for *confounding* **and** *effect modification*.

---

# What do we put into our model?

* Do we put in **all** the data?

* Do we include **only** what we _know_ is likely to be useful?

* Do we carry out some sort of stepwise approach?

---

# What do we put into our model?

* Do we put in **all** the data?

* Do we include **only** what we _know_ is likely to be useful?

* Do we carry out some sort of stepwise approach?

<font size="46">We should avoid all of these!</font>

---

# Do's and do not's

*The do's*

Expert knowledge

- Think about the research question you are trying to answer
 - Statistics should follow a theory not the other way round

---

# Do's and do not's

*The do's*

Expert knowledge

- Think about the research question you are trying to answer
 - Statistics should follow a theory not the other way round
- If you, or the literature, thinks something is important add it!
 - A non-significant result in these is a useful result

---
# Do's and do not's

*The do not's*

Stepwise variable selection

 - This is no longer recommended for several reasons
 - It can remove useful and retain useless variables due to coincidence rather than real world meaning.
 
---

# Do's and do not's

*The do not's*

Stepwise variable selection

 - This is no longer recommended for several reasons
 - It can remove useful and retain useless variables due to coincidence rather than real world meaning.
 
Screening using a univariate regression

- Non-significant variables can still affect parameter estimates
- This can reject important variables when there is confounding or effect modification

---

# Do's and do not's

*The do not's*

This is only a brief summary, please read further into the topic with this blog post for further information (https://towardsdatascience.com/stopping-stepwise-why-stepwise-selection-is-bad-and-what-you-should-use-instead-90818b3f52df)

---

# Overfitting

This is where the model "memorises" a pattern rather than "learns" it. 

This can give us a model that performs very well on the dataset that it has been given, but very poorly on a new dataset.

---

# Overfitting

```{r, eval = TRUE, echo = FALSE, out.width = "100%"}
knitr::include_graphics(xfun::relative_path(here::here("images", "regression", "overfitting.png")))
```
https://www.educative.io/api/edpresso/shot/6668977167138816/image/5033807687188480
---

# Overfitting 

Even if we are not predicting our model to a new dataset, we want to know that the conclusions our model is coming to are valid.

To do this we want to **train** our model on one subset of data, and **test** our model on a different subset of data.

If we find the same associations in both datasets we can have more confidence that we have found a real association.

---

# Lasso variable selection

If you are presented with a large number of variables, and there is some uncertainty in what is important, you can use a technique called Lasso (Least Absolute Shrinkage and Selection Operator) regression.

Lasso regression works by testing variable and data combinations to create a model which has the **least** difference in predictions between the **in-sample** and the **out-of-sample** datasets.

---

# Lasso variable selection

Lasso regression works by testing variable and data combinations to create a model which has the **least** difference in predictions between the **in-sample** and the **out-of-sample** datasets.

---

# A simple model is a good model!

[Occam's razor (the law of parsimony)](https://en.wikipedia.org/wiki/Occam%27s_razor) - Do not make something more complicated that is necessary.

A simple model is a good model
 - As long as it can still adequately explain our research questions.

---

#Lasso variable selection

1) First we set up the lasso regression using the function `trainControl()` from the **caret** package.

.smaller[
```{r, echo = T, eval = T}
fitControl <- trainControl(
     method = "repeatedcv", # We want to use repeated cross validation
     number = 5,            # Divide the dataset into 5 sections to train and test
     repeats = 10)          # Repeat this with 10 "cuts" of data
```
]
???
Talk through each of method/number/repeats

method - This stands for repeated cross validation, the process of taking out a "chunk" of the dataset and trying to predict it using the rest of the dataset. This is done to make sure the model is not overfitting

number - Split the data into 5 sections (20% removed each time) - the smaller the number the larger the "chunks" in the dataset. This makes the fitting harder to do, but can help increase the validity of predictions

repeats - Repeat 10 times with different "cuts" of the data to ensure the result we get is a true relationship and not just do to a "lucky" or "unlucky" split of the data

---

# Cross validation

.smaller[
```{r, echo = T, eval = T}
fitControl <- trainControl(
     method = "repeatedcv", # We want to use repeated cross validation
     number = 5,            # Divide the dataset into 5 sections to train and test
     repeats = 10)          # Repeat this with 10 "cuts" of data
```
]

```{r, eval = TRUE, echo = FALSE, out.width = "100%"}
knitr::include_graphics(xfun::relative_path(here::here("images", "regression", "KfoldCV.gif")))
```

???

Talk through this gif in the context of the last slide

---

# Run the lasso regression

1) First we subset our data to the potential independent variables of interest, and the dependent variable.


.smaller[
```{r, echo = T, eval = T}

data_use <- linelist %>%
    select(gender,                           #These are the exposures
           age,                              #want to investigate
           hospital:bmi, 
           blood_ct, 
           outcome_death)

```
]
---

# Run the lasso regression

1) First we subset our data to the potential independent variables of interest, and the dependent variable.

2) Then we remove na values with `drop_na()`.

.smaller[
```{r, echo = T, eval = T}

data_use <- linelist %>%
    select(gender,                           #These are the exposures
           age,                              #want to investigate
           hospital:bmi, 
           blood_ct, 
           outcome_death) %>%
     drop_na()                                #We can only use complete rows

```
]
---

# Run the lasso regression

1) First we subset our data to the potential independent variables of interest, and the dependent variable.

2) Then we remove na values with `drop_na()`.

3) Then we use `train()` from **caret** to run the lasso regression (in caret we specify `method = "glmnet"`) according to the conditions we specified with `trainControl()`

.smaller[
```{r, echo = T, eval = T}

data_use <- linelist %>%
    select(gender,                           #These are the exposures
           age,                              #want to investigate
           hospital:bmi, 
           blood_ct, 
           outcome_death) %>%
     drop_na()                                #We can only use complete rows

lasso_fit <- train(form = outcome_death ~ .,  
                   data = data_use,
                   method = "glmnet",       #This is the caret name for lasso 
                   trControl = fitControl)  #This is the training method we specified

```
]
---

# Lasso results

Then we use the function `predictors()` from **caret** to 

```{r, echo = T, eval = T}
lasso_predictors_raw <- predictors(lasso_fit)

lasso_predictors_raw
```

???
Talk through these predictors and why some have "hospital" in front of them (factor and then the level that is important)

---

# Lasso results

We can then process these to get them ready for use using `str_extract()` from the **stringi** package.

This takes our results and removes everything _not_ found in the column names in our dataset.

.smaller[
```{r, echo = T, eval = T}
lasso_predictors_clean <- str_extract(string = lasso_predictors_raw,
                                      pattern = paste(colnames(data_use),   
                                             collapse = "|"))
lasso_predictors_clean
```
]
---

# Expert knowledge (biological plausibility) is important

Are we done now, did lasso solve our model building?

---

# Expert knowledge (biological plausibility) is important

Are we done now, did lasso solve our model building?

<font size="46">No!</font>

If there is a variable that you think should be included (based on biological plausibility or a previously established association), include it!

---

# Carrying out our multivariable regression

Now we are ready to carry out our multivariable regression!

This is done in a slightly different way to our univariate regression. We will be using **base** R's `glm()` function and then tidying up the outputs using the function `tbl_regression` from **gtsummary**.

---

#Carrying out multivariable regression

```{r, echo = T, eval = F}

multivariable_regression_table <- data_use %>%
     select(lasso_predictors_clean, 
            outcome_death,
            temp,
            age) %>%


```

???
Talk through the stages, what formula means, what family means, remind them what exponentiate = TRUE means

---

#Carrying out multivariable regression

```{r, echo = T, eval = F}

multivariable_regression_table <- data_use %>%
     select(lasso_predictors_clean, 
            outcome_death,
            temp,
            age) %>%
     glm(formula = outcome_death ~.,     
         family = binomial)

```

???
Talk through the stages, what formula means, what family means, remind them what exponentiate = TRUE means

---

#Carrying out multivariable regression

```{r, echo = T, eval = F}

multivariable_regression_table <- data_use %>%
     select(lasso_predictors_clean, 
            outcome_death,
            temp,
            age) %>%
     glm(formula = outcome_death ~.,     
         family = binomial) %>%            
     tbl_regression(exponentiate = TRUE)

```

???
Talk through the stages, what formula means, what family means, remind them what exponentiate = TRUE means

---

#Carrying out multivariable regression

```{r, echo = T, eval = F}

multivariable_regression_table <- data_use %>%
     select(lasso_predictors_clean, 
            outcome_death,
            temp,
            age) %>%
     glm(formula = outcome_death ~.,     
         family = binomial) %>%            
     tbl_regression(exponentiate = TRUE)

#Create a table of counts
cross_tab <- data_use %>%
  tbl_summary(by = outcome_death)

```

???
Talk through the stages, what formula means, what family means, remind them what exponentiate = TRUE means

---

#Carrying out multivariable regression

```{r, echo = T, eval = F}

multivariable_regression_table <- data_use %>%
     select(lasso_predictors_clean, 
            outcome_death,
            temp,
            age) %>%
     glm(formula = outcome_death ~.,     
         family = binomial) %>%            
     tbl_regression(exponentiate = TRUE)

#Create a table of counts
cross_tab <- data_use %>%
  select(lasso_predictors_clean, 
            outcome_death,
            temp,
            age) %>%
  tbl_summary(by = outcome_death)

#Combine the regression table and the count table
tbl_merge(list(cross_tab, multivariable_regression_table),
          tab_spanner = c("Counts", "Multivariable regression"))
     

```

???
Talk through the stages, what formula means, what family means, remind them what exponentiate = TRUE means

---
#Carrying out multivariable regression

```{r, echo = F, eval = T}

multivariable_regression_table <- data_use %>%
     select(lasso_predictors_clean, 
            outcome_death,
            temp,
            age) %>%
     glm(formula = outcome_death ~.,     
         family = binomial) %>%            
     tbl_regression(exponentiate = TRUE)

cross_tab <- data_use %>%
  select(lasso_predictors_clean, 
            outcome_death,
            temp,
            age) %>%
  tbl_summary(by = outcome_death)

tbl_merge(list(cross_tab, multivariable_regression_table),
          tab_spanner = c("Counts", "Multivariable regression"))
     

```

---
# Displaying results

You can also use `tbl_merge` to compare and contrast your univariate and multivariable regression results.

.tiny[
```{r, echo = F, eval = T, out.height = "300px", out.width = "700px"}
univariate_regression_table <- data_use %>%
    select(c(lasso_predictors_clean, 
             outcome_death,
             temp,
             age)) %>%
    tbl_uvregression(
    method = glm,
    y = outcome_death,
    method.args = list(family = binomial),
    exponentiate = TRUE)

tbl_merge(tbls = list(univariate_regression_table, multivariable_regression_table),
          tab_spanner = c("Univariate", "Multivariable"))
```
]
---

# Displaying results

.pull-left[
And you can create forest plots using `ggplot()`
]

.pull-right[
```{r, echo = F, eval = T, out.height = "400px", out.width = "600px"}
forest_plot_dataset <- bind_rows(univariate_regression_table$table_body %>%
                                       mutate(regression_type = "univariate"),
                             multivariable_regression_table$table_body %>%
                                  mutate(regression_type = "multivariable"))

#Now we create the forest plot
ggplot(data = forest_plot_dataset %>%
            filter(!is.na(estimate)), 
       aes(x = estimate,
           y = label,
           xmin = conf.low,
           xmax = conf.high,
           color = regression_type)) +
     geom_point(position = position_dodge(width = 0.5)) +
     geom_errorbarh(position = position_dodge(width = 0.5)) +
     theme_bw() +
     labs(y = "Variable", 
          x = "Odds ratio", 
          color = "Regression type",
          subtitle = "Odds ratio estimates for predictors of Ebola death\nfrom uni- and multivariable regression") +
     geom_vline(xintercept = 1, 
                linetype = "dashed")

```
]
---

# It's now time for you to try it out yourselves!

Any questions?



**Resources**

Course website (initial setup and slides access): [https://appliedepi.github.io/intro_course/](https://appliedepi.github.io/intro_course/)

[Epi R Handbook](epirhandbook.com/)

Applied Epi Community
https://community.appliedepi.org/









